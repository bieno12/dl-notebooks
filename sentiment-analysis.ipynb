{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeyad/miniconda3/envs/my-torch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchsummary\n",
    "import torchtext\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics import Accuracy, Precision\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "\n",
    "import os, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_mapping = {\"positive\": 0, \"neutral\": 1, \"negative\": 2}\n",
    "\n",
    "train_df = pd.read_csv(\"data/tweet-sentiment-extraction/train.csv\").drop(columns=['textID'])\n",
    "train_df['text'] = train_df['text'].str.lower().fillna('')\n",
    "train_df['selected_text'] = train_df['selected_text'].str.lower()\n",
    "train_df['sentiment'] = train_df['sentiment'].apply(lambda x: sentiment_mapping[x])\n",
    "\n",
    "test_df = pd.read_csv(\"data/tweet-sentiment-extraction/test.csv\").drop(columns=['textID'])\n",
    "test_df['text'] = test_df['text'].str.lower().fillna('')\n",
    "test_df['sentiment'] = test_df['sentiment'].apply(lambda x: sentiment_mapping[x])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded encoded text from cache.\n",
      "Loaded encoded text from cache.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1045,  1036,  ...,     0,     0,     0],\n",
       "        [  101, 17111,  2080,  ...,     0,     0,     0],\n",
       "        [  101,  2026,  5795,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  8038,  2100,  ...,     0,     0,     0],\n",
       "        [  101,  2021,  2009,  ...,     0,     0,     0],\n",
       "        [  101,  2035,  2023,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "def get_encoded_text(df, tokenizer, cache_path):\n",
    "    # Check if the cache file exists\n",
    "    if os.path.exists(cache_path):\n",
    "        # Load the encoded text from cache\n",
    "        encoded_data = torch.load(cache_path)\n",
    "        print(\"Loaded encoded text from cache.\")\n",
    "        return encoded_data\n",
    "    else:\n",
    "        # Encode the text and save it to cache\n",
    "        encoded_sentences = df['text'].apply(lambda x: tokenizer.encode_plus(\n",
    "            x,\n",
    "            add_special_tokens=True,       # Add '[CLS]' and '[SEP]'\n",
    "            max_length=128,                # Pad & truncate all sentences to max length\n",
    "            padding='max_length',          # Pad all sentences to max length\n",
    "            truncation=True,               # Truncate long sentences to max length\n",
    "            return_attention_mask=False,    # Return attention mask\n",
    "            return_tensors='pt'            # Return pytorch tensors\n",
    "        ))\n",
    "        \n",
    "        # Convert the list of encoded sentences to a dictionary of tensors\n",
    "        input_ids = torch.cat([item['input_ids'] for item in encoded_sentences])\n",
    "        \n",
    "        # Save the tensors to cache\n",
    "        encoded_data = input_ids\n",
    "        torch.save(encoded_data, cache_path)\n",
    "        print(\"Encoded text and saved to cache.\")\n",
    "        return encoded_data\n",
    "\n",
    "# Example usage with train and test DataFrames\n",
    "train_encoded = get_encoded_text(train_df, tokenizer, \"train_encoded_sentences.pt\")\n",
    "test_encoded = get_encoded_text(test_df, tokenizer, \"test_encoded_sentences.pt\")\n",
    "\n",
    "train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from minbpe import RegexTokenizer\n",
    "# tokenizer = RegexTokenizer()\n",
    "# tokenizer.load(\"toy.model\")\n",
    "# vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "# def get_encoded_text(df,tokenizer, cache_path):\n",
    "#     # Check if the cache file exists\n",
    "#     if os.path.exists(cache_path):\n",
    "#         # Load the encoded text from cache\n",
    "#         encoded_sentences = torch.load(cache_path)\n",
    "#         print(\"Loaded encoded text from cache.\")\n",
    "#         return encoded_sentences\n",
    "#     else:\n",
    "#         # Encode the text and save it to cache\n",
    "#         encoded_sentences = df['text'].apply(lambda x: tokenizer.encode(x))\n",
    "#         torch.save(encoded_sentences, cache_path)\n",
    "#         print(\"Encoded text and saved to cache.\")\n",
    "#         return encoded_sentences\n",
    "#     return encoded_sentences\n",
    "\n",
    "# train_encoded : pd.Series = get_encoded_text(train_df, tokenizer, \"train_encoded_sentences.pt\")\n",
    "# test_encoded : pd.Series = get_encoded_text(test_df, tokenizer, \"test_encoded_sentences.pt\")\n",
    "\n",
    "\n",
    "# class TextDataset(Dataset):\n",
    "#     def __init__(self, encoded_texts: pd.Series, labels: pd.Series, max_seq_length: int):\n",
    "#         self.max_seq_length = max_seq_length\n",
    "#         self.encoded_texts = self.pad_and_truncate(encoded_texts)\n",
    "#         self.labels = torch.tensor(labels.values, dtype=torch.long)\n",
    "\n",
    "#     def pad_and_truncate(self, encoded_texts: pd.Series) -> torch.Tensor:\n",
    "#         padded_texts = []\n",
    "#         for encoded_text in encoded_texts:\n",
    "#             if len(encoded_text) > self.max_seq_length:\n",
    "#                 padded_text = encoded_text[:self.max_seq_length]\n",
    "#             else:\n",
    "#                 padded_text = encoded_text + [0] * (self.max_seq_length - len(encoded_text))\n",
    "#             padded_texts.append(padded_text)\n",
    "#         return torch.tensor(padded_texts, dtype=torch.long)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.encoded_texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         encoded_text = self.encoded_texts[idx]\n",
    "#         label = self.labels[idx]\n",
    "#         return encoded_text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass an appropriate PyTorch class \n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Initialize the positional encoding matrix\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Calculate and assign position encodings to the matrix\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    # Update the embeddings tensor adding the positional encodings\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\tdef __init__(self, d_model: int, nums_heads: int):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\n",
    "\t\tself.num_heads = nums_heads\n",
    "\t\tself.d_model = d_model\n",
    "\t\tself.head_dim = d_model // nums_heads\n",
    "\n",
    "\t\tself.query_linear = nn.Linear(d_model, d_model)\n",
    "\t\tself.key_linear = nn.Linear(d_model, d_model)\n",
    "\t\tself.value_linear = nn.Linear(d_model, d_model)\n",
    "\t\tself.output_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "\tdef split_heads(self, x: torch.Tensor, batch_size: int):\n",
    "\t\tx = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "\n",
    "\t\t#rearrange for batched matmul\n",
    "\t\treturn x.permute(0, 2, 1, 3).contiguous()\\\n",
    "\t\t\t.view(batch_size * self.num_heads, -1, self.head_dim)\n",
    "\t\t\n",
    "\tdef compute_attention(self, query: torch.Tensor, key: torch.Tensor, mask=None):\n",
    "\t\tscores = torch.matmul(query, key.transpose(-2, -1)) * self.d_model**-.5\n",
    "\n",
    "\t\tif mask is not None:\n",
    "\t\t\tscores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\t\tattention_weights = torch.softmax(scores, dim=-1)\n",
    "\t\treturn attention_weights\n",
    "\t\n",
    "\tdef forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask=None):\n",
    "\t\tbatch_size = query.size(0)\n",
    "\n",
    "\t\tquery = self.split_heads(self.query_linear(query), batch_size)\n",
    "\t\tkey = self.split_heads(self.key_linear(key), batch_size)\n",
    "\t\tvalue = self.split_heads(self.value_linear(value), batch_size)\n",
    "\n",
    "\t\tattention_weights = self.compute_attention(query, key, mask)\n",
    "\n",
    "\t\toutput = torch.matmul(attention_weights, value)\n",
    "\n",
    "\t\toutput = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "\t\treturn self.output_linear(output)\n",
    "\t\n",
    "class FeedForwardSubLayer(nn.Module):\n",
    "\tdef __init__(self, d_model, d_ff):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.model = nn.Sequential(\n",
    "\t\t\tnn.Linear(d_model, d_ff),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(d_ff, d_model),\n",
    "\t\t)\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\tdef __init__(self, d_model, num_heads, d_ff, dropout) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\t\n",
    "\n",
    "\t\tself.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "\t\tself.feedforward = FeedForwardSubLayer(d_model, d_ff)\n",
    "\t\tself.norm1 = nn.LayerNorm(d_model)\n",
    "\t\tself.norm2 = nn.LayerNorm(d_model)\n",
    "\t\tself.dropoutLayer = nn.Dropout(dropout)\n",
    "\n",
    "\tdef forward(self, x, mask):\n",
    "\t\toutputs = self.self_attention(x, x, x)\n",
    "\t\tx = self.norm1(x + self.dropoutLayer(outputs))\n",
    "\n",
    "\t\tff_outputs = self.feedforward(x)\n",
    "\n",
    "\t\tx = self.norm2(x + self.dropoutLayer(ff_outputs))\n",
    "\t\treturn x\n",
    "\t\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "\tdef __init__(self,\n",
    "\t\t\tvocab_size: int,\n",
    "\t\t\td_model: int,\n",
    "\t\t\tnum_layers: int,\n",
    "\t\t\tnum_heads: int,\n",
    "\t\t\td_ff: int,\n",
    "\t\t\tdropout: float,\n",
    "\t\t\tmax_seq_length: int\n",
    "\t\t):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\t\tself.positional_encoding = PositionalEncoder(d_model, max_seq_length)\n",
    "\t\tself.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "\tdef forward(self, x, mask):\n",
    "\t\tx = self.embedding(x)\n",
    "\t\tx = self.positional_encoding(x)\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tx = layer(x, mask)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        # Add linear layer for multiple-class classification\n",
    "        self.fc = nn.Linear(d_model,  num_classes)\n",
    "\n",
    "    def forward(self, x,):\n",
    "        logits = self.fc(x[:, 0, :])\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تعليم بنية نموذج المحول علي معلومات تويتات و تصنيفها من حيث الشعور\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "batch_size = 32\n",
    "d_model = 256\n",
    "num_heads = 32\n",
    "num_layers = 3\n",
    "d_ff = 4 * d_model\n",
    "sequence_length = 128\n",
    "dropout = 0.5\n",
    "\n",
    "train_dataset = TensorDataset(train_encoded, torch.tensor(train_df['sentiment'].values))\n",
    "test_dataset = TensorDataset(test_encoded, torch.tensor(test_df['sentiment'].values))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "encoder = TransformerEncoder(\n",
    "\tvocab_size=vocab_size,\n",
    "\td_model=d_model,\n",
    "\tnum_layers=num_layers,\n",
    "\tnum_heads=num_heads,\n",
    "\td_ff=d_ff,\n",
    "\tdropout=dropout,\n",
    "\tmax_seq_length=sequence_length\n",
    "\t).to(device)\n",
    "classifier = ClassifierHead(d_model, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 859/859 [01:00<00:00, 14.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training LossZ = 1.0885120265975126, Validation Loss = 0.9857209856445724, Validation Accuracy = 0.5121675133705139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 859/859 [00:59<00:00, 14.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training LossZ = 0.8470303180678482, Validation Loss = 0.8310850676115569, Validation Accuracy = 0.6129032373428345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 859/859 [00:59<00:00, 14.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training LossZ = 0.7152467985855409, Validation Loss = 0.7611070731738666, Validation Accuracy = 0.6598755121231079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 859/859 [00:59<00:00, 14.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training LossZ = 0.6059964245772889, Validation Loss = 0.7498277490203445, Validation Accuracy = 0.6743067502975464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 859/859 [00:59<00:00, 14.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Training LossZ = 0.5183625632378774, Validation Loss = 0.8006908936543508, Validation Accuracy = 0.6717600226402283\n"
     ]
    }
   ],
   "source": [
    "accuracy_metric = Accuracy(task='multiclass', num_classes=num_classes).to(device)  # Ensure the metric is on the same device as your models\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=0.0001)\n",
    "# optimizer = torch.optim.Adam(list(encoder.parameters()), lr=0.0001)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # encoder.train()\n",
    "    encoder.eval()\n",
    "    classifier.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    accuracy_metric.reset()\n",
    "    # Training phase\n",
    "    for sequences, labels in tqdm(train_dataloader):\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        \n",
    "        encoded_sequences = encoder(sequences, mask=mask)\n",
    "        logits = classifier(encoded_sequences)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass and optimization step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        accuracy_metric.update(logits, labels)\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    train_accuracy = accuracy_metric.compute()\n",
    "    accuracy_metric.reset()\n",
    "\n",
    "    # Validation phase\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_dataloader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            \n",
    "            encoded_sequences = encoder(sequences, mask=mask)\n",
    "            logits = classifier(encoded_sequences)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            accuracy_metric(logits, labels)\n",
    "    val_accuracy = accuracy_metric.compute()\n",
    "    val_loss /= len(test_dataloader)\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Training LossZ = {epoch_loss / len(train_dataloader)}, Validation Loss = {val_loss}, Validation Accuracy = {val_accuracy}\")\n",
    "    accuracy_metric.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeyad/miniconda3/envs/my-torch/lib/python3.12/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Training Loss = 0.5701334268826128, Validation Loss = 0.17670556902885437, Validation Accuracy = 0.7080000042915344\n",
      "Confusion Matrix:\n",
      "          Positive  Neutral  Negative\n",
      "Positive       210       99        18\n",
      "Neutral         29      313        58\n",
      "Negative         8       80       185\n",
      "\n",
      "Top 10 Misclassified Examples (ordered by loss):\n",
      "Sequence: oh! i ate pizza last night too! i stupidly feel closer to you somehow!, True Label: Positive, Predicted Label: Negative, Loss: 5.4190545082092285\n",
      "Sequence: getting my phone back this week yeeeewww, True Label: Positive, Predicted Label: Neutral, Loss: 5.2540602684021\n",
      "Sequence: prom is so over rated!! irritated... going to bed goodnight, True Label: Negative, Predicted Label: Positive, Loss: 4.931923866271973\n",
      "Sequence: the hotel should be thankful, True Label: Positive, Predicted Label: Neutral, Loss: 4.784107208251953\n",
      "Sequence: _ other excellent analogy, True Label: Positive, Predicted Label: Neutral, Loss: 4.611337661743164\n",
      "Sequence: thankfully that face only shows up for photoshoots, True Label: Positive, Predicted Label: Neutral, Loss: 4.566944599151611\n",
      "Sequence: wango tango!!! good night all, True Label: Neutral, Predicted Label: Positive, Loss: 4.5130462646484375\n",
      "Sequence: keeping it chilled., True Label: Positive, Predicted Label: Neutral, Loss: 4.458378791809082\n",
      "Sequence: yes it does, True Label: Positive, Predicted Label: Neutral, Loss: 4.451817512512207\n",
      "Sequence: yep. nothing to worry about., True Label: Positive, Predicted Label: Neutral, Loss: 4.394593715667725\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "\n",
    "confusion_matrix_metric = ConfusionMatrix(task='multiclass', num_classes=num_classes).to(device)\n",
    "\n",
    "# Validation phase\n",
    "encoder.eval()\n",
    "classifier.eval()\n",
    "val_loss = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "misclassified_examples = []\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=1000)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sequences, labels = next(iter(test_dataloader))\n",
    "    sequences, labels = sequences.to(device), labels.to(device)\n",
    "    \n",
    "    encoded_sequences = encoder(sequences, mask=mask)\n",
    "    logits = classifier(encoded_sequences)\n",
    "    loss = F.cross_entropy(logits, labels, reduce=False)\n",
    "    \n",
    "    val_loss += loss.mean().item()\n",
    "    accuracy_metric(logits, labels)\n",
    "    \n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    all_preds.append(preds)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "    _, max_indices = torch.sort(loss, descending=True)\n",
    "    misclassified_examples.extend(zip(sequences[max_indices].tolist(), labels[max_indices], preds[max_indices].tolist(), loss[max_indices].tolist()))\n",
    "\n",
    "val_accuracy = accuracy_metric.compute()\n",
    "val_loss /= len(test_dataloader)\n",
    "\n",
    "# Stack all predictions and labels\n",
    "all_preds = torch.cat(all_preds)\n",
    "all_labels = torch.cat(all_labels)\n",
    "\n",
    "print(f\"Epoch {epoch}, Training Loss = {epoch_loss / len(train_dataloader)}, Validation Loss = {val_loss}, Validation Accuracy = {val_accuracy}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "confusion_matrix = confusion_matrix_metric(all_preds, all_labels)\n",
    "\n",
    "# Define labels\n",
    "labels = ['Positive', 'Neutral', 'Negative']\n",
    "\n",
    "# Convert confusion matrix to pandas DataFrame for better readability\n",
    "cm_df = pd.DataFrame(confusion_matrix.cpu().numpy(), index=labels, columns=labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_df)\n",
    "\n",
    "# Sort misclassified examples by loss values\n",
    "\n",
    "# Get top n misclassified examples\n",
    "top_n = 10  # Change to the desired value\n",
    "top_n_misclassified = misclassified_examples[:top_n]\n",
    "\n",
    "# Print top n misclassified examples\n",
    "print(f\"\\nTop {top_n} Misclassified Examples (ordered by loss):\")\n",
    "for sequence, true_label, pred_label, loss_value in top_n_misclassified:\n",
    "    print(f\"Sequence: {tokenizer.decode(sequence, True)}, True Label: {labels[true_label]}, Predicted Label: {labels[pred_label]}, Loss: {loss_value}\")\n",
    "\n",
    "# Reset metrics for next epoch\n",
    "accuracy_metric.reset()\n",
    "confusion_matrix_metric.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
